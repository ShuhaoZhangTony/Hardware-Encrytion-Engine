#include "crypto/algapi.h"
#include "linux/crypto.h"
#include "linux/interrupt.h"
#include "linux/io.h"
#include "linux/kthread.h"
#include "linux/platform_device.h"
#include "linux/scatterlist.h"
#include "linux/slab.h"
#include "linux/module.h"
#include "crypto/internal/hash.h"
#include "crypto/sha.h"
#include <linux/pci.h>
#include "mv_cesa.h"
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/pci.h>
#include <linux/pci_ids.h>
#include <linux/crypto.h>
#include <linux/spinlock.h>
#include <crypto/algapi.h>
#include <crypto/aes.h>
#include <linux/time.h>
#include <linux/io.h>
#include <linux/delay.h>
//#include "dsi-aes.h"
#define MV_CESA	"MV-CESA:"
#define MAX_HW_HASH_SIZE	0xFFFF
#define DMA_REGISTER_SIZE         (4 * DREGCOUNT)    // There are eighteen registers, and each is 4 bytes wide.



//#define DSI_DMA_ENABLE
// #define DSI_AES_ALG_ENABLE
#define DSI_AES_ECB_ENABLE
// #define DSI_AES_CBC_ENABLE


#define DSI_HW_CRYPT_VENDOR_ID		0x10EE
#define DSI_HW_CRYPT_DEVICE_ID		0x6018
#define DSI_HW_CRYPT_SUBVENDOR_ID 	0x10EE
#define DSI_HW_CRYPT_SUBDEVICE_ID 	0x0007

#define HAVE_REGION 0x01                    // I/O Memory region
#define HAVE_IRQ    0x02                    // Interupt
#define HAVE_KREG   0x04                    // Kernel registration

#define BUF_SIZE		4096

#define DMA_WRITE_START		0x00000001	
#define DMA_WR_RELAX_ORDER	0x00000020
#define DMA_WR_NO_SNOOP		0x00000040
#define DMA_WINTR_DISABLE 	0x00000080
#define DMA_WRITE_DONE		0x00000100

#define DMA_READ_START 		0x00010000
#define DMA_RD_RELAX_ORDER	0x00200000
#define DMA_RD_NO_SNOOP		0x00400000
#define DMA_RINTR_DISABLE	0x00800000
#define DMA_READ_DONE		0x01000000
#define maxOfCollector 1024*10
/* Static structures */
enum {
	DSCR,		 //Device control/status register 
	DDMACR,		 //DMA control register 
	WDMATLPA,	 //address of destination data buffer (device perspective)
	WDMATLPS,
	WDMATLPC,
	WDMATLPP,
	RDMATLPP,
	RDMATLPA,	 //address of source data buffer (device perspective) 
	RDMATLPS,
	RDMATLPC,
	WDMAPERF,
	RDMAPERF,
	RDMASTAT,
	NRDCOMP,
	RCOMPDSIZW,
	DLWSTAT,
	DLTRSSTAT,
	DMISCCONT,
	
	AESLENCR,	 //AES length & control register; BIT31 1=encrypt, 0=decrypt; BIT0-15 number of bytes to encrypt 
	AESKEY0_0,	 //AES key MSW 
	AESKEY0_1,
	AESKEY0_2,
	AESKEY0_3,	 //AES key LSW 

	DREGCOUNT
};
typedef struct
dma_mem_struct
{
	unsigned long addr;
	unsigned long size;
	void __iomem  *map;
}	dma_mem_t;
static dma_mem_t dsi_aes_dma;
static unsigned long stat_flags = 0;
static unsigned char *wr_buf = NULL, *rd_buf = NULL, *unaligned_buf = NULL;
static dma_addr_t rd_hw_addr, wr_hw_addr;
/*
 * STM:
 *   /-----------------------------------------------------\
 *   |			|-----------|					  			| 
 *  \./			\./		 	|					 			|
 * IDLE -> new request->collect 8 -> BUSY -> done -> DEQUEUE  request complete
 *			   
 *			    
 */
enum engine_status {
	ENGINE_IDLE,
	ENGINE_BUSY,
	ENGINE_W_DEQUEUE,
};

/**
 * struct req_progress - used for every crypt request
 * @src_sg_it:		sg iterator for src
 * @dst_sg_it:		sg iterator for dst
 * @sg_src_left:	bytes left in src to process (scatter list)
 * @src_start:		offset to add to src start position (scatter list)
 * @crypt_len:		length of current hw crypt/hash process
 * @hw_nbytes:		total bytes to process in hw for this request
 * @copy_back:		whether to copy data back (crypt) or not (hash)
 * @sg_dst_left:	bytes left dst to process in this scatter list
 * @dst_start:		ofset to add to dst start position (scatter list)
 * @hw_processed_bytes:	number of bytes processed by hw (request).
 *
 * sg helper are used to iterate over the scatterlist. Since the size of the
 * SRAM may be less than the scatter size, this struct is used to keep
 * track of progress within current scatterlist.
 */
struct req_progress {
	struct sg_mapping_iter src_sg_it;
	struct sg_mapping_iter dst_sg_it;
	void (*complete) (int id);
	void (*process) (int is_first);

	/* src mostly */
	int sg_src_left;
	int src_start;
	int hw_nbytes;//total bytes to process in hw for this request
	/* dst mostly */
	int sg_dst_left;
	int dst_start;
	//I am assume the hardware will finish hw_nbytes everytime 
	//instead of finish it in multiple cycle.
	// which is a wrong assumtion.
};

struct crypto_priv {
	struct crypto_async_request *request_collector[maxOfCollector];//this array is used to store address of request
	int ID;
	int numOfTask;
	int firstFlag;
	int IFlag_newCrypt;//this can also being used to represent stopOfI
	int totalRequestSize;
	struct task_struct *queue_th;
	int finishOfQ;
	int stopOfQ;
	struct task_struct *interrupt_th;
	int finishOfI;
	/* the lock protects queue and eng_st */	
	enum engine_status eng_st;
	struct req_progress p;
};

static struct crypto_priv *cpg[2];

struct common_struct {
	spinlock_t lock;
	struct crypto_queue queue;//com own the queue	
	unsigned char sourceBuf[4096];
	unsigned char destBuf[4096];
	unsigned char *sPointer;
	unsigned char *dPointer;
	int counter;
	struct crypto_async_request **taskPointer;	
};
static struct common_struct *com;

struct mv_ctx {
	u8 aes_enc_key[AES_KEY_LEN];
	u32 aes_dec_key[8];
	int key_len;
	u32 need_calc_aes_dkey;
};

enum crypto_op {
	COP_AES_ECB,
	//COP_AES_CBC,
};

struct mv_req_ctx {
	enum crypto_op op;
	int decrypt;
};
struct timespec start, end;
struct timespec switch_start, switch_end;
static inline void
_writefield(u32 offset, void *value)
{
#if 1 
	int i;
	for (i = 0; i < 4; i++)
		iowrite32(((u32 *) value)[i], dsi_aes_dma.map + offset + (i * 4));
#endif
}

/* Read a 128 bit field (either a writable key or IV) */
static inline void
_readfield(u32 offset, void *value)
{
#if 1
	int i;
	for (i = 0; i < 4; i++)
		((u32 *) value)[i] = ioread32(dsi_aes_dma.map + offset + (i * 4));
#endif
}
static DECLARE_WAIT_QUEUE_HEAD(wq);
static int flag = 0;
static void dequeue_complete_req(struct crypto_priv *localCpg,
			struct crypto_async_request **local_taskPointer);
static void DataPrepare(struct crypto_priv *localCpg,int i,
			struct crypto_async_request **local_taskPointer);
static void do_crypt(void *src, void *dst, int len, u32 flags);//one char by char

int crypto_aes_expand_key_generic(struct crypto_aes_ctx *ctx, const u8 *in_key,
		unsigned int key_len);
int crypto_aes_set_key_generic(const u8 *in_key,
		unsigned int key_len);
void aes_encrypt_generic(u8 *out, const u8 *in);
void aes_decrypt_generic(u8 *out, const u8 *in);
static int interrupt_manag(struct crypto_priv *localCpg)
{	
	int numOfRun=0;
	int dataSize=0;
	int id=localCpg->ID;
	printk(KERN_INFO "First time in interrupt manager %d",id);
	do{
		__set_current_state(TASK_INTERRUPTIBLE);
		printk(KERN_INFO "interrupt manager%d is running",id);
		int i=999;
		while(i>0){
		msleep(1000);
		i--;
		}
		//from wake up I is not finished
		localCpg->finishOfI=0;
		wait_event_interruptible_timeout(wq,localCpg->IFlag_newCrypt==0,MAX_SCHEDULE_TIMEOUT);
		if(localCpg->IFlag_newCrypt){
				com->taskPointer=localCpg->request_collector;
				while(localCpg->numOfTask>0){
					if(localCpg->numOfTask>8){
					numOfRun=8;
					}else{
					numOfRun=localCpg->numOfTask;
					}
					localCpg->numOfTask-=numOfRun;
					//printk(KERN_INFO "start data prepare");
					//getnstimeofday(&start);
					DataPrepare(localCpg,numOfRun,com->taskPointer);
					//getnstimeofday(&end);
					//long difference = end.tv_nsec - start.tv_nsec;
					//printk ("%ld in DataPrepare indexOfCollector is %d \n", difference,i);
			     	//getnstimeofday(&start);
					if(localCpg->totalRequestSize>4096){
					dataSize=4096;				
					}else{
					dataSize=localCpg->totalRequestSize;
					}
					localCpg->totalRequestSize-=dataSize;
					printk(KERN_INFO "dataSize is %d, localCpg->totalRequestSize is %d",dataSize,localCpg->totalRequestSize);
					do_crypt(com->sourceBuf, com->destBuf, dataSize, localCpg->firstFlag);
					//getnstimeofday(&end);
					//difference = end.tv_nsec - start.tv_nsec;
					printk(KERN_INFO "PROCESSDONE");
					//printk(KERN_INFO "deque_comple being called!");	
					//getnstimeofday(&start);
					dequeue_complete_req(localCpg,com->taskPointer);
					//getnstimeofday(&end);
		     			//difference = end.tv_nsec - start.tv_nsec;
					//printk ("%ld for dequeue_complete_req is %d \n", difference,i);
					printk ("finish dequeue_complete_req");
					com->taskPointer+=numOfRun;
				}
				localCpg->IFlag_newCrypt=0;
				localCpg->totalRequestSize=0;
			}

		//getnstimeofday(&switch_start);
		localCpg->eng_st = ENGINE_IDLE;
		//till here I is finished
		localCpg->finishOfI=1;
		
		wait_event_interruptible_timeout(wq,cpg[id^0x01]->finishOfQ==0,MAX_SCHEDULE_TIMEOUT);
		wake_up_interruptible(&wq);
		//getnstimeofday(&switch_end);
	   	//long difference = switch_end.tv_nsec - switch_start.tv_nsec;
		//printk ("\n %ld in switching from Q to I \n", difference);
	} while (!kthread_should_stop());
	//printk(KERN_INFO "Exiting interrupt Manager.");
	return 0;
}

static int queue_manag(struct crypto_priv *localCpg)
{
	struct crypto_async_request *async_req = NULL;
	struct crypto_async_request *backlog = NULL;
	struct crypto_queue *queue=&com->queue;
	struct ablkcipher_request *req= NULL;
	struct mv_req_ctx *req_ctx=NULL;
	struct crypto_async_request **cPointer;//collector pointer point to the array of pointer
	int forceProcess=0;
	int maxData=512*maxOfCollector;//assuming each request have 512 bytes data
	int id=localCpg->ID;//0 or 1
	localCpg->eng_st = ENGINE_IDLE;
	cPointer=localCpg->request_collector;
	printk(KERN_INFO "First time in queue manager %d",id);
	if(id==1)
	{
		printk(KERN_INFO "Q1 is scheduled before do while");
		__set_current_state(TASK_INTERRUPTIBLE);
		wait_event(wq,cpg[0]->finishOfQ==0);
		printk(KERN_INFO "Q1 is waking up before do while");
	}
	do {		
		__set_current_state(TASK_INTERRUPTIBLE);
		msleep(10000);
		if (localCpg->eng_st == ENGINE_IDLE) {
			printk(KERN_DEBUG "eng_st of Q%d is idle",id);
			spin_lock_irq(&com->lock);
			backlog = crypto_get_backlog(&com->queue);
			async_req = crypto_dequeue_request(&com->queue);
			spin_unlock_irq(&com->lock);			
			if (async_req) {			
				req= ablkcipher_request_cast(async_req);	
				printk(KERN_INFO "request coming size is %d",req->nbytes);
				req_ctx= ablkcipher_request_ctx(req);	
				BUG_ON(localCpg->eng_st != ENGINE_IDLE);
				localCpg->totalRequestSize += req->nbytes;
				printk(KERN_INFO "total data size is %d",localCpg->totalRequestSize);
				*cPointer=async_req;/*address in async_req copied into request_collector */	
				cPointer++;
				printk(KERN_INFO"cPointer is point to somewhere of request_collector address is %p",cPointer);
				if(localCpg->numOfTask==0) {
					localCpg->firstFlag=req_ctx->decrypt;
				}	
				localCpg->numOfTask++;	
				if(localCpg->totalRequestSize >= maxData || req_ctx->decrypt != localCpg->firstFlag){//operation change
					forceProcess=1;
					localCpg->eng_st = ENGINE_BUSY;//or task is 8 already will not ask dequeue anymore.
				}
			}
			else //async_req is null
			{//no more request
				printk(KERN_INFO "Q%d checking no more request",id);
				if(localCpg->numOfTask>0){//if currently does have tasks.
					localCpg->eng_st = ENGINE_BUSY;
					forceProcess=1;
				}
				//else just do nothing.
			}
		}
		if (backlog) {
			backlog->complete(backlog, -EINPROGRESS);
			backlog = NULL;
		}
		if(forceProcess)
		{	
			printk(KERN_INFO "++++++queue man is going to sleep localCpg->numOfTask is %d \n",localCpg->numOfTask);
			localCpg->finishOfQ=1;
			if(id==0)
			wait_event_interruptible_timeout(wq,cpg[1]->finishOfI==0,MAX_SCHEDULE_TIMEOUT);
			else
			wait_event_interruptible_timeout(wq,cpg[0]->finishOfI==0,MAX_SCHEDULE_TIMEOUT);

			printk(KERN_INFO "start I%d \n",id);	
			localCpg->IFlag_newCrypt=1;
			//getnstimeofday(&switch_start);
			//printk(KERN_WARNING "wake up interrupt manager \n");
			wake_up_interruptible(&wq);
			if(id==0)
			wait_event_interruptible_timeout(wq,cpg[1]->finishOfQ==0,MAX_SCHEDULE_TIMEOUT);
			else
			wait_event_interruptible_timeout(wq,cpg[0]->finishOfQ==0,MAX_SCHEDULE_TIMEOUT);

			localCpg->finishOfQ=0;
			//during this time, higher layer will 
			cPointer = localCpg->request_collector;
			printk(KERN_INFO"cPointer of %d is point to starting of request_collector address is %p",id,cPointer);			
			//getnstimeofday(&switch_end);
			//long difference = switch_end.tv_nsec - switch_start.tv_nsec;
			//printk ("\n !!!%ld in switch from I to Q \n", difference);
			//try to max the queue again
			//printk(KERN_WARNING "------Should not being called after +");
			forceProcess=0;	
		}
		if(queue->qlen==0&&localCpg->numOfTask==0){
			printk(KERN_INFO "nothing to handle sleep");	
			localCpg->stopOfQ=1;
			wait_event_interruptible_timeout(wq,queue->qlen==0,100000);//no more request to process
			localCpg->finishOfQ=0;
			cPointer = localCpg->request_collector;
			printk(KERN_INFO"cPointer of Q%d is point to starting of request_collector address is %p",id,cPointer);		
		}
	} while (!kthread_should_stop());
	printk(KERN_INFO "Exiting Queue Manager.");
	return 0;
}

static inline void do_crypt(void *src, void *dst, int len, u32 flags)
{
	u8 *src_walk = (u8 *)src;
	u8 *dst_walk = (u8 *)dst;
	do {
		if (!flags)
			aes_encrypt_generic(dst_walk, src_walk);
		else
			aes_decrypt_generic(dst_walk, src_walk);
		len -= AES_MIN_BLOCK_SIZE;
		src_walk += AES_MIN_BLOCK_SIZE;
		dst_walk += AES_MIN_BLOCK_SIZE;
	} while (len);
}

static inline int count_sgs(struct scatterlist *sl, unsigned int total_bytes)
{
	int i = 0;
	size_t cur_len;

	while (sl) {
		cur_len = sl[i].length;
		++i;
		if (total_bytes > cur_len)
			total_bytes -= cur_len;
		else
			break;
	}

	return i;
}
static int mv_handle_req(struct crypto_async_request *req)
{
	unsigned long flags;
	int ret;
	struct crypto_queue *queue=&com->queue;
	ret = crypto_enqueue_request(&com->queue, req);
	spin_unlock_irqrestore(&com->lock, flags);
	printk(KERN_INFO "!!!!!!enqueue being called, queue size is %d \n",queue->qlen);
	if(cpg[0]->stopOfQ==1&&cpg[1]->stopOfQ==1&&cpg[0]->IFlag_newCrypt==0&&cpg[1]->IFlag_newCrypt==0)
	wake_up(&wq);
	return ret;
}
static int mv_enc_aes_ecb(struct ablkcipher_request *req)
{
	int flags;
	spin_lock_irqsave(&com->lock, flags);
	struct mv_req_ctx *req_ctx = ablkcipher_request_ctx(req);
	req_ctx->op = COP_AES_ECB;
	req_ctx->decrypt = 0;
	return mv_handle_req(&req->base);
}
static inline void compute_aes_dec_key(struct mv_ctx *ctx)
{
	struct crypto_aes_ctx gen_aes_key;
	int key_pos;

	if (!ctx->need_calc_aes_dkey)
		return;

	crypto_aes_expand_key(&gen_aes_key, ctx->aes_enc_key, ctx->key_len);

	key_pos = ctx->key_len + 24;
	memcpy(ctx->aes_dec_key, &gen_aes_key.key_enc[key_pos], 4 * 4);
	switch (ctx->key_len) {
	case AES_KEYSIZE_256:
		key_pos -= 2;
		/* fall */
	case AES_KEYSIZE_192:
		key_pos -= 2;
		memcpy(&ctx->aes_dec_key[4], &gen_aes_key.key_enc[key_pos],
				4 * 4);
		break;
	}
	ctx->need_calc_aes_dkey = 0;
}
static int mv_dec_aes_ecb(struct ablkcipher_request *req)
{
	int flags;
	spin_lock_irqsave(&com->lock, flags);
	struct mv_ctx *ctx = crypto_tfm_ctx(req->base.tfm);
	struct mv_req_ctx *req_ctx = ablkcipher_request_ctx(req);
	struct crypto_queue *queue=&com->queue;
	//printk(KERN_INFO "dec aes ecb method being called");
	req_ctx->op = COP_AES_ECB;
	req_ctx->decrypt = 1;
	compute_aes_dec_key(ctx);
	//printk(KERN_INFO "dec aes ecb method being alled:The size of queue is %d",queue->qlen);
	return mv_handle_req(&req->base);
}
static inline void mv_crypto_algo_completion(int id)
{
	sg_miter_stop(&cpg[id]->p.src_sg_it);
	sg_miter_stop(&cpg[id]->p.dst_sg_it);
}
static inline void DataPrepare(struct crypto_priv *localCpg,int i,
			struct crypto_async_request **local_taskPointer)	
{
	struct req_progress *p = &localCpg->p;
	struct ablkcipher_request *req;
	int num_sgs;
	int ret;
	void *sbuf;
	int copy_len;
	int len;
	char *dbuf;
	com->sPointer=com->sourceBuf;
	com->dPointer=com->destBuf;
	memset(p, 0, sizeof(struct req_progress));//initial p to 0
	p->complete = mv_crypto_algo_completion;//static void mv_crypto_algo_completion(int)
	//p->process = mv_process_current_q;
	while(i>0)
	{
		printk(KERN_INFO "in dataPrepare crypto_async_request %p", *local_taskPointer);	
		req =ablkcipher_request_cast(*local_taskPointer);
		local_taskPointer++;
		printk(KERN_INFO "in dataPrepare ablkcipher_request %p", req);	
		p->hw_nbytes = req->nbytes;//
		num_sgs = count_sgs(req->src, req->nbytes);
		printk(KERN_INFO "req->nbytes is %d",req->nbytes);
		sg_miter_start(&p->src_sg_it, req->src, num_sgs, SG_MITER_FROM_SG);
		com->counter++;
		i--;
		printk(KERN_WARNING "entering copy_src_to_buf");
		dbuf=com->sPointer;
		len=p->hw_nbytes;
		while (len) 
		{
			if (!p->sg_src_left) 
			{
				ret = sg_miter_next(&p->src_sg_it);
				BUG_ON(!ret);
				p->sg_src_left = p->src_sg_it.length;
				p->src_start = 0;
			}
				sbuf = p->src_sg_it.addr + p->src_start;
				copy_len = min(p->sg_src_left, len);
				memcpy(dbuf, sbuf, copy_len);
				p->src_start += copy_len;
				p->sg_src_left -= copy_len;
				len -= copy_len;
				dbuf += copy_len;
		} 
		com->sPointer += p->hw_nbytes;
	}
	printk(KERN_INFO "Exit data prepare method: total data size is %d ",localCpg->totalRequestSize);
}

static inline void dequeue_complete_req(struct crypto_priv *localCpg,
			struct crypto_async_request **local_taskPointer)	
{
	struct ablkcipher_request *req; 
	struct crypto_async_request *asreq;
	struct req_progress *p = &localCpg->p;
	int num_sgs;
	int ret;
	int offset;
	int dst_copy;
	void *buf;	
	int len;
	char *sbuf;
	while(com->counter> 0) {
		req =ablkcipher_request_cast(*local_taskPointer);
		asreq=*local_taskPointer;
		local_taskPointer++;
		p->hw_nbytes = req->nbytes;//
		num_sgs = count_sgs(req->dst, req->nbytes);
		sg_miter_start(&p->dst_sg_it, req->dst, num_sgs, SG_MITER_TO_SG);	
		
		offset = 0;	
		sbuf=com->dPointer;
		len=p->hw_nbytes;	
		do {
			if (!localCpg->p.sg_dst_left) {
				ret = sg_miter_next(&localCpg->p.dst_sg_it);
				BUG_ON(!ret);
				localCpg->p.sg_dst_left = localCpg->p.dst_sg_it.length;
				localCpg->p.dst_start = 0;
			}
			buf = localCpg->p.dst_sg_it.addr;
			buf += localCpg->p.dst_start;
			dst_copy = min(len, localCpg->p.sg_dst_left);
			memcpy(buf,
			       sbuf + offset,
			       dst_copy);
			offset += dst_copy;
			localCpg->p.sg_dst_left -= dst_copy;
			len -= dst_copy;
			localCpg->p.dst_start += dst_copy;
		} while (len > 0);
		
		com->dPointer+= p->hw_nbytes;
		com->counter--;
		local_bh_disable();
		asreq->complete(asreq, 0);
		local_bh_enable();				
		localCpg->p.complete(localCpg->ID);
	}	
}
static int mv_cra_init(struct crypto_tfm *tfm)
{
	tfm->crt_ablkcipher.reqsize = sizeof(struct mv_req_ctx);
	return 0;
}
/* CRYPTO-API Functions */
//notes, the API function input paramater has small difference with dsi aes
static int mv_setkey_aes(struct crypto_ablkcipher *cipher, const u8 *key,
		unsigned int len)
{
	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);
	struct mv_ctx *ctx = crypto_tfm_ctx(tfm);
	ctx->key_len = len;
	ctx->need_calc_aes_dkey = 1;
	//printk(KERN_WARNING "dsi_setkey_blk: setting key, len = %d.", len);
	crypto_aes_set_key_generic(key, len);
//	printk(KERN_INFO "crypto set key being called");
	memcpy(ctx->aes_enc_key, key, AES_KEY_LEN);
	return 0;
	
}
struct crypto_alg mv_aes_alg_ecb = {
	.cra_name		= "ecb(aes)",
	.cra_driver_name	= "mv-ecb-aes",
	.cra_priority	= 300,
	.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
	.cra_blocksize	= 16,
	.cra_ctxsize	= sizeof(struct mv_ctx),
	.cra_alignmask	= 0,
	.cra_type	= &crypto_ablkcipher_type,
	.cra_module	= THIS_MODULE,
	.cra_init	= mv_cra_init,
	.cra_u		= {
		.ablkcipher = {
			.min_keysize	=	AES_MIN_KEY_SIZE,
			.max_keysize	=	AES_MAX_KEY_SIZE,
			.setkey		=	mv_setkey_aes,
			.encrypt	=	mv_enc_aes_ecb,
			.decrypt	=	mv_dec_aes_ecb,
		},
	},
};

static int __devinit
dsi_aes_probe(struct pci_dev *dev, const struct pci_device_id *id)
{
}

static void __devexit dsi_aes_remove(struct pci_dev *dev)
{
	crypto_unregister_alg(&mv_aes_alg_ecb);
	if (stat_flags & HAVE_REGION)
		release_mem_region(dsi_aes_dma.addr, DMA_REGISTER_SIZE);
	iounmap(dsi_aes_dma.map);
	dsi_aes_dma.map = NULL;

	pci_release_regions(dev);
	pci_disable_device(dev);
}
static struct pci_device_id 
dsi_aes_tbl[] = 
{
	{
		.vendor	=	DSI_HW_CRYPT_VENDOR_ID,
		.device	=	DSI_HW_CRYPT_DEVICE_ID,
		.subvendor =	DSI_HW_CRYPT_SUBVENDOR_ID,
		.subdevice =	DSI_HW_CRYPT_SUBDEVICE_ID,
	},
	{0, }
};
static struct pci_driver dsi_aes_driver = {
	.name = "DSI AES Driver",
	.id_table = dsi_aes_tbl,
	.probe = dsi_aes_probe,
	.remove = __devexit_p(dsi_aes_remove)
};

MODULE_ALIAS("platform:mv_crypto");

static int __init dsi_crypto_init(void)
{
	//return pci_register_driver(&dsi_aes_driver);
int ret;

#ifdef DSI_AES_ECB_ENABLE
        printk(KERN_WARNING "dsi-aes: registering dsi_ecb_alg.\n");
        ret = crypto_register_alg(&mv_aes_alg_ecb);
        if (ret)
                goto ealg;
#endif
        printk(KERN_WARNING "dsi-aes: DSI AES engine enabled.\n");
		com = kzalloc(sizeof(struct common_struct), GFP_KERNEL);
		if (!com)
			return -ENOMEM;
		cpg[0] = kzalloc(sizeof(struct crypto_priv), GFP_KERNEL);
		if (!cpg[0])
			return -ENOMEM;
		cpg[0]->ID=0;
		cpg[0]->finishOfQ=0;
		cpg[0]->finishOfI=0;
		cpg[1] = kzalloc(sizeof(struct crypto_priv), GFP_KERNEL);
		if (!cpg[1])
			return -ENOMEM;
		cpg[1]->ID=1;
		cpg[1]->finishOfQ=0;
		cpg[1]->finishOfI=0;
		spin_lock_init(&com->lock);
		crypto_init_queue(&com->queue,maxOfCollector);
		printk(KERN_WARNING "queue initialized\n");
				
		cpg[0]->queue_th = kthread_run(queue_manag, cpg[0], "mv_crypto");
		printk(KERN_WARNING "queue manager 1 started\n");
		cpg[0]->interrupt_th = kthread_run(interrupt_manag, cpg[0], "mv_crypto");
		printk(KERN_WARNING "interrupt manager 1 started\n");
		
		cpg[1]->queue_th = kthread_run(queue_manag, cpg[1], "mv_crypto");
		printk(KERN_WARNING "queue manager 2 started\n");
		cpg[1]->interrupt_th = kthread_run(interrupt_manag, cpg[1], "mv_crypto");
		printk(KERN_WARNING "interrupt manager 2 started\n");
		
		        return 0;
 eecb:
#ifdef DSI_AES_ECB_ENABLE
        crypto_unregister_alg(&mv_aes_alg_ecb);
#endif
 ealg:
#ifdef DSI_AES_ALG_ENABLE
       // crypto_unregister_alg(&dsi_alg);
#endif
 eexit:
        printk(KERN_WARNING "dsi-aes: DSI AES loading failed.\n");
}
module_init(dsi_crypto_init);

static void __exit dsi_crypto_exit(void)
{
	//pci_unregister_driver(&dsi_aes_driver);
#ifdef DSI_AES_ECB_ENABLE
        crypto_unregister_alg(&mv_aes_alg_ecb);
        //printk(KERN_WARNING "dsi-aes: DSI AES engine disabled.\n");
#endif
		kthread_stop(cpg[0]->queue_th);
		kthread_stop(cpg[0]->interrupt_th);
		kthread_stop(cpg[1]->queue_th);
		kthread_stop(cpg[1]->interrupt_th);
}

module_exit(dsi_crypto_exit);

MODULE_AUTHOR("Sebastian Andrzej Siewior <sebastian@breakpoint.cc>");
MODULE_DESCRIPTION("Support for Marvell's cryptographic engine");
MODULE_LICENSE("GPL");
